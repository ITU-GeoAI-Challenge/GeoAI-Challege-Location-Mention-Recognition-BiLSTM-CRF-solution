{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7fb8bfc-99ac-41a4-804c-0612991f89fa",
   "metadata": {},
   "source": [
    "__Challenge Link__: https://zindi.africa/competitions/geoai-challege-location-mention-recognition-from-social-media\n",
    "\n",
    "In the initial processing phase, I employed the Spacy en-core small model to tokenize the input texts and prepare them for model compatibility. I made slight adjustments to the text tokenizer. Here I have captured and retained essential information including the event name, Tweet ID, word, part of speech tag, start offset, and the word's target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b21e3e8-6446-4207-9911-a796db10ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import jsonlines\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_suffix_regex, compile_infix_regex\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5fd5d0-c667-4f79-a9a2-21421da9fbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nabar\\Documents\\Data_Science\\Challenge\\Zindi\\GeoAI Challege Location Mention Recognition from Social Media by ITU\\Submission\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d5f664-854d-4406-8f2a-308410be4e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nabar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.6.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the spaCy English tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a custom rule to split \"@\" as a separate token\n",
    "prefixes = list(nlp.Defaults.prefixes) + [r'@']\n",
    "suffixes = list(nlp.Defaults.suffixes) + [r'@']\n",
    "infixes = list(nlp.Defaults.infixes) + [r'@']\n",
    "custom_tokenizer = Tokenizer(nlp.vocab, prefix_search=compile_prefix_regex(prefixes).search,\n",
    "                              suffix_search=compile_suffix_regex(suffixes).search,\n",
    "                              infix_finditer=compile_infix_regex(infixes).finditer,\n",
    "                              token_match=None)\n",
    "\n",
    "# Update the spaCy tokenizer with the custom tokenizer\n",
    "nlp.tokenizer = custom_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b520b0c1-5cbc-483b-895f-b63267e188e4",
   "metadata": {},
   "source": [
    "# Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "056dc3d7-2308-4876-ad78-66b27ad64f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_data_path = current_directory + r'\\IDRISI-main\\LMR\\data\\EN\\gold-random-json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec24310-a2fc-4958-9df9-e522e658ec99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\nabar\\\\Documents\\\\Data_Science\\\\Challenge\\\\Zindi\\\\GeoAI Challege Location Mention Recognition from Social Media by ITU\\\\Submission\\\\IDRISI-main\\\\LMR\\\\data\\\\EN\\\\gold-random-json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ed153ad-8808-4651-ae9c-67d22d4d4523",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_event_list = glob.glob(gold_data_path + '/*/train.jsonl', recursive = True)\n",
    "val_event_list = glob.glob(gold_data_path + '/*/dev.jsonl', recursive = True)\n",
    "test_event_list = glob.glob(gold_data_path + '/*/test_unlabeled.jsonl', recursive = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feea69f9-fde9-44bd-b3cc-c0b1ec9893eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_event_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b277ee82-bbaa-4349-95d8-decc71b1691b",
   "metadata": {},
   "source": [
    "# Create Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "787afe8b-e90a-49bd-ac35-6e96a021507d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 16s\n",
      "Wall time: 3min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize the result as a list of lists\n",
    "result = []\n",
    "\n",
    "for event in train_event_list:\n",
    "    event_name = event.split('\\\\')[-2]\n",
    "    with jsonlines.open(event, \"r\") as reader:\n",
    "        for input_json in reader:\n",
    "            sentence_id = input_json[\"tweet_id\"]\n",
    "            text = input_json[\"text\"]\n",
    "            location_mentions = input_json[\"location_mentions\"]\n",
    "            # Tokenize the text using spaCy\n",
    "            doc = nlp(text)\n",
    "            # Process the tokenized text and location mentions\n",
    "            if len(location_mentions)>0:\n",
    "                for token in doc:\n",
    "                    start_offset = token.idx\n",
    "                    end_offset = start_offset + len(token.text)\n",
    "                    tag = \"O\"\n",
    "                    for item in location_mentions:\n",
    "                        start_index_gold = item['start_offset']\n",
    "                        end_index_gold = item['end_offset']\n",
    "                        if start_offset >=start_index_gold and end_offset<=end_index_gold:\n",
    "                            tag = \"LOC\"\n",
    "                    result.append([event_name, sentence_id, str(token.text), str(token.pos_), start_offset, tag])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34668ce4-e295-4ae4-a960-99449fefc1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "columns = [\"Event_Name\", \"Sentence\", \"Word\", \"POS\", \"start_offset\", \"Tag\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(result, columns=columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.to_csv('train.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be792bae-4b46-4492-a691-81bf7261e355",
   "metadata": {},
   "source": [
    "# Create Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "995aef2d-7683-4881-980f-25903d752cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 16.1 s\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize the result as a list of lists\n",
    "result = []\n",
    "\n",
    "for event in val_event_list:\n",
    "    event_name = event.split('\\\\')[-2]\n",
    "    with jsonlines.open(event, \"r\") as reader:\n",
    "        for input_json in reader:\n",
    "            sentence_id = input_json[\"tweet_id\"]\n",
    "            text = input_json[\"text\"]\n",
    "            location_mentions = input_json[\"location_mentions\"]\n",
    "            # Tokenize the text using spaCy\n",
    "            doc = nlp(text)\n",
    "            # Process the tokenized text and location mentions\n",
    "            if len(location_mentions)>0:\n",
    "                for token in doc:\n",
    "                    start_offset = token.idx\n",
    "                    end_offset = start_offset + len(token.text)\n",
    "                    tag = \"O\"\n",
    "                    for item in location_mentions:\n",
    "                        start_index_gold = item['start_offset']\n",
    "                        end_index_gold = item['end_offset']\n",
    "                        if start_offset >=start_index_gold and end_offset<=end_index_gold:\n",
    "                            tag = \"LOC\"\n",
    "                    result.append([event_name, sentence_id, str(token.text), str(token.pos_), start_offset, tag])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07e05494-557f-485a-854c-08fddf9818ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "columns = [\"Event_Name\", \"Sentence\", \"Word\", \"POS\", \"start_offset\", \"Tag\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(result, columns=columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.to_csv('val.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08086867-3d3e-4af7-875b-21f57f670498",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efeaa67e-1c09-4fb0-a3f0-073780a86a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 39 s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize the result as a list of lists\n",
    "result = []\n",
    "\n",
    "for event in test_event_list:\n",
    "    event_name = event.split('\\\\')[-2]\n",
    "    with jsonlines.open(event, \"r\") as reader:\n",
    "        for input_json in reader:\n",
    "            sentence_id = input_json[\"tweet_id\"]\n",
    "            text = input_json[\"text\"]\n",
    "            # Tokenize the text using spaCy\n",
    "            doc = nlp(text)\n",
    "            # Process the tokenized text and location mentions\n",
    "            for token in doc:\n",
    "                start_offset = token.idx\n",
    "                result.append([event_name, sentence_id, token.text, token.pos_, start_offset])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "780a7a21-189b-439f-8685-7cc4d863e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "columns = [\"Event_Name\", \"Sentence\", \"Word\", \"POS\", \"start_offset\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(result, columns=columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.to_csv('test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3a2cd56-9ff1-44a3-a05d-25ac4848ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7b0709-86e1-40af-8e65-f6c317402870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
